{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spooky2.ipynb",
      "provenance": [],
      "mount_file_id": "1tgpZ3UK5g_YczCbqdK8ImsG6QqDxlK34",
      "authorship_tag": "ABX9TyOATkXLmleHJgOQADJnUtsA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karma-os/Kaggle_study/blob/main/Spooky2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4U2CA79JEIG"
      },
      "source": [
        "## Approaching Any NLP Problem on kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFVGYoBvQSXN",
        "outputId": "c9fb9f35-18c7-497f-fc6c-b0c89db9ec1c"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8znlAbctQTIe"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import Embedding, BatchNormalization\n",
        "from tensorflow.keras import utils\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from tensorflow.keras.layers import GlobalMaxPool1D, Conv1D, MaxPool1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing import sequence, text\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPno0_miQTOO"
      },
      "source": [
        "import os\n",
        "need= \"drive/MyDrive/Kaggle_kernel/data/spooky\"\n",
        "\n",
        "if os.getcwd() == \"/content\":\n",
        "    chdir = os.path.join(os.getcwd(),need)\n",
        "    os.chdir(need)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcXtCL8tUy6C",
        "outputId": "e2e838e1-bbb2-48e1-95a0-3f8036fc180c"
      },
      "source": [
        "#Linux switch command : \n",
        "\"\"\"\n",
        "-f  freshen existing files, create none\n",
        "-n  never overwrite existing files         \n",
        "-q  quiet mode (-qq => quieter)\n",
        "-o  overwrite files WITHOUT prompting \n",
        "\"\"\"\n",
        "\n",
        "!unzip -o train.zip \n",
        "!unzip -o test.zip\n",
        "!unzip -o sample_submission.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  train.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  sample_submission.zip\n",
            "  inflating: sample_submission.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRXqJbWkQTQb"
      },
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test  = pd.read_csv(\"test.csv\")\n",
        "sample = pd.read_csv(\"sample_submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBGrnQBBQTTE"
      },
      "source": [
        "# Text clf with 3 diff cases.\n",
        "\"\"\"\n",
        "Multi-class log loss\n",
        "\"\"\"\n",
        "\n",
        "def multiclass_logloss(actual, predicted, eps = 1e-15):\n",
        "    \"\"\"Multiclass version of log-loss matric.\n",
        "    :param actual: Array containing the actual target classes\n",
        "    :param predicted: Matrix with clas predictions, one prob per class\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert 'actual' to binary if it's not already:\n",
        "    if len(actual.shape) == 1:\n",
        "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
        "        for i,val in enumerate(actual):\n",
        "            actual2[i,val] = 1\n",
        "        actual = actual2\n",
        "    clip = np.clip(predicted, eps, 1-eps)\n",
        "    rows = actual.shape[0]\n",
        "    vsota = np.sum(actual * np.log(clip))\n",
        "    return -1.0 / rows * vsota \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlEUGZ-KQTVc"
      },
      "source": [
        "#Label encoding to convert text labels to 0,1,2\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(train.author.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r8b2LlHQTZ9"
      },
      "source": [
        "xtrain, xvalid, ytrain,yvalid = train_test_split(train.text.values, y,\n",
        "                                                 stratify = y,\n",
        "                                                 random_state = 42,\n",
        "                                                 test_size = 0.1, \n",
        "                                                 shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNeO2F3JQTcE",
        "outputId": "523642d0-5025-40bd-825d-d2a8e9a253a1"
      },
      "source": [
        "print(xtrain.shape)\n",
        "print(xvalid.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17621,)\n",
            "(1958,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ayf9HwQTec"
      },
      "source": [
        "# Building Basic Models\n",
        "\n",
        "**TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "3V_9Gr5YXhUO",
        "outputId": "30735b90-02b5-470e-d533-e79924793b05"
      },
      "source": [
        "#help(TfidfVectorizer)\n",
        "\n",
        "\"\"\"\n",
        "Params 복습\n",
        "\n",
        "min_df\n",
        "\n",
        "max_features\n",
        "\n",
        "strip_accent : Remove accents and perform character normalization\n",
        "\n",
        "analyzer : Feature made of word or n-grams?\n",
        "\n",
        "token_pattern : '\\w{1,}'\n",
        "\n",
        "-> \\w  : used to find a word character(a-z, A-Z, 0-9, character)\n",
        "이런 w를 metacharacter라고 하나봐.\n",
        "\n",
        "{n,m} : preceding character가 n~m회 반복.\n",
        "\n",
        "그러면 결국 이 Token pattern은 word character가 1회 이상인 것.\n",
        "\n",
        "디폴트 옵션은 \\b\\w\\w+\\b 라고 한다.\n",
        "\n",
        "\n",
        "####\n",
        "The default regexp selects tokens of 2\n",
        " or more alphanumeric characters (punctuation is completely ignored\n",
        " and always treated as a token separator).\n",
        "####\n",
        "\n",
        "\n",
        "ngram_range : ex) (1,3) : unigram, bigram, 3-gram\n",
        "\n",
        "use_idf : Enable IDF reweighting.\n",
        "\n",
        "smooth_idf : Prevents zero division\n",
        "\n",
        "sublinear_tf : Replace tf with 1 + log(tf)\n",
        "\n",
        "stop_words\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nParams 복습\\n\\nmin_df\\n\\nmax_features\\n\\nstrip_accent : Remove accents and perform character normalization\\n\\nanalyzer : Feature made of word or n-grams?\\n\\ntoken_pattern : '\\\\w{1,}'\\n\\n-> \\\\w  : used to find a word character(a-z, A-Z, 0-9, character)\\n이런 w를 metacharacter라고 하나봐.\\n\\n-> {n,m} : preceding character가 n~m회 반복.\\n\\n그러면 결국 이 Token pattern은 word character가 1회 이상인거겠지.\\n\\n\\n####\\nThe default regexp selects tokens of 2\\n or more alphanumeric characters (punctuation is completely ignored\\n and always treated as a token separator).\\n####\\n\\n\\nngram_range : ex) (1,3) : unigram, bigram, 3-gram\\n\\nuse_idf : Enable IDF reweighting.\\n\\nsmooth_idf : Prevents zero division\\n\\nsublinear_tf : Replace tf with 1 + log(tf)\\n\\nstop_words\\n\\n\\n\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p3fY4HPQTg7"
      },
      "source": [
        "# Always start with these features. They work (almost) everytime!\n",
        "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
        "            stop_words = 'english')\n",
        "\n",
        "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
        "tfv.fit(list(xtrain) + list(xvalid))\n",
        "xtrain_tfv =  tfv.transform(xtrain) \n",
        "xvalid_tfv = tfv.transform(xvalid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca5CLW4pQTjV",
        "outputId": "f7f6de99-d123-4e59-8601-cead2a943fb5"
      },
      "source": [
        "# Fitting a simple Logistic Regression on TFIDF\n",
        "clf = LogisticRegression(C=1.0, max_iter = 500)\n",
        "clf.fit(xtrain_tfv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_tfv)\n",
        "\n",
        "print(\"Case : LR on TFIDF\")\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Case : LR on TFIDF\n",
            "logloss: 0.572 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2uBkgafQTlz"
      },
      "source": [
        " **Let's improve it**\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgjv-NjoQToF"
      },
      "source": [
        "ctv = CountVectorizer(analyzer = \"word\", token_pattern = r\"\\w{1,}\",\n",
        "                      ngram_range = (1,3), stop_words = \"english\")\n",
        "\n",
        "#Fitting CountVectorizer to training, test sets\n",
        "\n",
        "ctv.fit(list(xtrain)+list(xvalid))\n",
        "xtrain_ctv = ctv.transform(xtrain)\n",
        "xvalid_ctv = ctv.transform(xvalid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23FS500deN25",
        "outputId": "bcedaddc-27ad-4546-f5d1-c4556f57d81a"
      },
      "source": [
        "clf = LogisticRegression(C=1.0, max_iter = 500)\n",
        "\n",
        "clf.fit(xtrain_ctv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_ctv)\n",
        "\n",
        "print(\"Case : LR on CountVectorizer\")\n",
        "print(\"logloss: %0.3f\" %multiclass_logloss(yvalid,predictions))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Case : LR on CountVectorizer\n",
            "logloss: 0.527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHlpc2iueN5g",
        "outputId": "65a1b915-e352-4b5c-bcb7-c2ea9a5146d7"
      },
      "source": [
        "#Fitting a simple NB on TFIDF\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(xtrain_tfv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_tfv)\n",
        "\n",
        "print(\"Case: NB on TFIDF\")\n",
        "print(\"logloss: %0.3f\" %multiclass_logloss(yvalid,predictions))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Case: NB on TFIDF\n",
            "logloss: 0.578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjqZLLTxeN8E",
        "outputId": "66ebff39-0a61-4bab-c662-074c1f1591a9"
      },
      "source": [
        "#Fitting a simple NB on CountVetcorizer\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(xtrain_ctv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_ctv)\n",
        "\n",
        "print(\"Case : NB on CountVectorizer\")\n",
        "print(\"logloss: %0.3f\" %multiclass_logloss(yvalid,predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Case : NB on CountVectorizer\n",
            "logloss: 0.485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRbVsSk5eN-d"
      },
      "source": [
        "**SVM**\n",
        "Some people love SVM, so we'll do SVM.\n",
        "\n",
        "Before SVm, we'll perform\n",
        "\n",
        "SVD(for computation)\n",
        "Standarization(compulsory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g46Jz-W0eOA1"
      },
      "source": [
        "#Apply SVD. 120~200 components are good enough\n",
        "\n",
        "svd = decomposition.TruncatedSVD(n_components = 120)\n",
        "svd.fit(xtrain_tfv)\n",
        "xtrain_svd = svd.transform(xtrain_tfv)\n",
        "xvalid_svd = svd.transform(xvalid_tfv)\n",
        "\n",
        "#Scale the data from SVD. Rename Variable to reuse without sacling\n",
        "\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(xtrain_svd)\n",
        "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
        "xvalid_svd_scl = scl.transform(xvalid_svd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SndYLOHUeODD"
      },
      "source": [
        "#Apply SVM.\n",
        "clf = SVC(C= 1.0,probability = True)\n",
        "clf.fit(xtrain_svd_scl, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_svd_scl)\n",
        "\n",
        "print(\"Case : SVM on TFIDF\")\n",
        "print(\"logloss: %0.3f\" %multiclass_logloss(yvalid, predictions))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx6z53SJeOFb"
      },
      "source": [
        "# Fitting simple xgb on tf-idf\n",
        "\n",
        "clf = xgb.XGBClassifier(max_depth = 7, n_estimators = 200, \n",
        "                        colsample_bytree = 0.8, subsample = 0.8, nthread = 10, learning_rate = 0.1)\n",
        "clf.fit(xtrain_tfv.tocsc(),ytrain)\n",
        "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
        "\n",
        "\n",
        "#tocsc(): compressed sparse column format\n",
        "\n",
        "print(\"Case : XGB on TFIDF\")\n",
        "print(\"logloss: %0.3f\" %multiclass_logloss(yvalid, predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms8XQXFHeOHz"
      },
      "source": [
        "# Fitting simple xgb on count\n",
        "\n",
        "clf = xgb.XGBClassifier(max_depth = 7, n_estimators = 200, \n",
        "                        colsample_bytree = 0.8, subsample = 0.8, nthread = 10, learning_rate = 0.1)\n",
        "clf.fit(xtrain_ctv.tocsc(),ytrain)\n",
        "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
        "\n",
        "\n",
        "#tocsc(): compressed sparse column format\n",
        "\n",
        "print(\"Case : XGB on TFIDF\")\n",
        "print(\"logloss: %0.3f\" %multiclass_logloss(yvalid, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWWkdbfBeOKN"
      },
      "source": [
        "# Fitting a simple xgboost on tf-idf svd features\n",
        "\n",
        "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "clf.fit(xtrain_svd, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_svd)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAydwUHyeOMm"
      },
      "source": [
        "# Fitting a simple xgboost on tf-idf svd features\n",
        "clf = xgb.XGBClassifier(nthread=10)\n",
        "clf.fit(xtrain_svd, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_svd)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFbkGDf9eORb"
      },
      "source": [
        "**Grid Search**\n",
        "\n",
        "Parameter looping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVWohpnUeOWU"
      },
      "source": [
        "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better= False, needs_proba = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmoBIdZNQTqj"
      },
      "source": [
        "#pipeline : SVD -> Scaling -> LR\n",
        "\n",
        "#Initialize svd\n",
        "\n",
        "svd = TruncatedSVD()\n",
        "\n",
        "#Initialize standard scaler\n",
        "\n",
        "scl = preprocessing.StandardScaler()\n",
        "\n",
        "#We'll use LR here.\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "#create Pipeline.\n",
        "\n",
        "clf = pipeline.Pipeline([(\"svd\",svd), (\"scl\",scl),(\"lr\",lr_model)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0EnW95UQTsr"
      },
      "source": [
        "#Param grid\n",
        "\n",
        "param_grid = {\"svd__n_components\":[120,180], \"lr__C\":[0.1,1.0,10], \"lr__penalty\": ['l1','l2']}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4502fuioBbt"
      },
      "source": [
        "#Initialize Grid Search model\n",
        "\n",
        "model = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = mll_scorer,\n",
        "                     verbose=10, n_jobs = -1, iid= True, refit = True, cv=2)\n",
        "\n",
        "\n",
        "#Fit Grid Search model\n",
        "\n",
        "model.fit(xtrain_tfv, ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtvGTQVEoBd9"
      },
      "source": [
        "print(\"Best score: %0.3f\" %model.best_score_)\n",
        "print(\"Best params set:\")\n",
        "best_parameters = model.best_estimator_.get_params()\n",
        "\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t{0}: {1}\".format(param_name, best_parameters[param_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gy5J_egoBgE"
      },
      "source": [
        "**LR tuning Score: similar to SVM ones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_ALXBqPpUvv"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLl73u8_oBiY"
      },
      "source": [
        "nb_model = MultinomialNB()\n",
        "\n",
        "\n",
        "clf = pipeline.Pipeline([('nb',nb_model)])\n",
        "\n",
        "# parameter grid\n",
        "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Initialize Grid Search Model\n",
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
        "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
        "\n",
        "# Fit Grid Search Model\n",
        "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \n",
        "print(\"Best score: %0.3f\" % model.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = model.best_estimator_.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMSmaXwWoBnL"
      },
      "source": [
        "**NB Grid search on TF : 0.57 -> 0.49**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJbFMteHoBwW"
      },
      "source": [
        "#Word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1_lzWXWoBzB",
        "outputId": "fe878dba-8b1e-4310-d44c-71fce06f8176"
      },
      "source": [
        "!unzip -f glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  glove.840B.300d.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozqUMSqCwshv",
        "outputId": "ae8b6577-7af4-4174-8352-c0a7eba13b22"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glove.840B.300d.txt  sample_submission.csv  test.csv  train.csv\n",
            "glove.840B.300d.zip  sample_submission.zip  test.zip  train.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEqpGCmFoB1Y",
        "outputId": "282c9d7c-267a-4399-c064-ff321c217e97"
      },
      "source": [
        "#Load Glove vectors in a dictionary:\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(\"glove.840B.300d.txt\")\n",
        "for line in tqdm(f):\n",
        "    try:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embeddings_index[word] = coefs\n",
        "    except:\n",
        "        f.__next__()\n",
        "f.close()\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2195997it [03:47, 9641.58it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2195864 word vectors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbNk-oztoB33"
      },
      "source": [
        "#CREATES NORMALIZED VECTOR FOR WHOLE SENTENCE\n",
        "\n",
        "def sent2vec(s):\n",
        "    words = str(s).lower()\n",
        "    words = word_tokenize(words)\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try: \n",
        "            M.append(embeddings_index[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis = 0)\n",
        "    if type(v)!=np.array:\n",
        "        return np.zeros(300)\n",
        "    return v/np.sqrt((v**2).sum())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voMhL_IeoB6f",
        "outputId": "e4501771-29c8-4954-ddbc-268c78d6dd85"
      },
      "source": [
        "#Create sentence ves using above ftn for train, val\n",
        "\n",
        "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
        "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 17621/17621 [00:06<00:00, 2886.46it/s]\n",
            "100%|██████████| 1958/1958 [00:00<00:00, 2949.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08FvOkDzoB9G"
      },
      "source": [
        "xtrain_glove = np.array(xtrain_glove)\n",
        "xvalid_glove = np.array(xvalid_glove)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yldtc_mHoB_3",
        "outputId": "9a48d569-6745-4ce1-fe3a-d49bf8dff3a3"
      },
      "source": [
        "#Fitting simple xgboost on glove features\n",
        "\n",
        "clf = xgb.XGBClassifier(nthread = 10, silent = False)\n",
        "clf.fit(xtrain_glove, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_glove)\n",
        "\n",
        "\n",
        "print(\"Case : Glove + XGBoost\")\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Case : Glove + XGBoost\n",
            "logloss: 1.088 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V04uC3QQoCCh",
        "outputId": "02f996e6-e141-4ea4-fff3-a90b641232c8"
      },
      "source": [
        "# Fitting a simple xgboost on glove features\n",
        "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
        "clf.fit(xtrain_glove, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_glove)\n",
        "\n",
        "print(\"Case: Glove + XGBoost other options \")\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Case: Glove + XGBoost other options \n",
            "logloss: 1.088 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgVofjbzoCFG"
      },
      "source": [
        "## Deep Learning Era\n",
        "\n",
        "LSTM, Simple Dense Network\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2sjcNScoCHw"
      },
      "source": [
        "#Scale the data before any neural net:\n",
        "\n",
        "scl = preprocessing.StandardScaler()\n",
        "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
        "xvalid_glove_scl  = scl.transform(xvalid_glove)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw-H6ysqZcNh"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl0WCi2MoCKG"
      },
      "source": [
        "#We need to binarize the label for NN\n",
        "\n",
        "ytrain_enc = to_categorical(ytrain)\n",
        "yvalid_enc = to_categorical(yvalid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmbkcNQKY1nM"
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovLrhlv_oCM6",
        "outputId": "87c2e9d8-461d-47aa-e2d6-85fcc3ffb2fe"
      },
      "source": [
        "#Create simple 3 layer Sequential neural net\n",
        "\n",
        "input_tensor = Input(shape = (300,), name = \"input\")\n",
        "x = Dense(300, activation = \"relu\", name = \"dense1\")(input_tensor)\n",
        "x = Dropout(0.2)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Dense(300,activation = \"relu\", name = \"dense2\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Dense(3)(x)\n",
        "\n",
        "output = Activation(\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs= input_tensor, outputs = output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 300)]             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 183,903\n",
            "Trainable params: 182,703\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXC7lKPhZ2b4"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",optimizer = \"adam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nXBYnwAoCPh",
        "outputId": "943327da-3414-44e7-d0c4-046fe2b0ad86"
      },
      "source": [
        "#model fitting\n",
        "\n",
        "model.fit(xtrain_glove_scl, y = ytrain_enc, batch_size = 64,\n",
        "          epochs = 5, verbose = 1,\n",
        "          validation_data=(xvalid_glove_scl, yvalid_enc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "276/276 [==============================] - 3s 6ms/step - loss: 1.0891 - val_loss: 1.0881\n",
            "Epoch 2/5\n",
            "276/276 [==============================] - 2s 5ms/step - loss: 1.0882 - val_loss: 1.0887\n",
            "Epoch 3/5\n",
            "276/276 [==============================] - 1s 5ms/step - loss: 1.0881 - val_loss: 1.0878\n",
            "Epoch 4/5\n",
            "276/276 [==============================] - 2s 6ms/step - loss: 1.0880 - val_loss: 1.0883\n",
            "Epoch 5/5\n",
            "276/276 [==============================] - 2s 5ms/step - loss: 1.0879 - val_loss: 1.0875\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7119d13b90>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKdL1bIvbPFh"
      },
      "source": [
        "## **Brief things about text.Tokenizer**\n",
        "\n",
        "* 1. 문자 기반 인코딩\n",
        "\n",
        "LISTEN -> L,I,S,T,E,N 아스키 이용하여 076, 073,...)\n",
        "\n",
        "listen 과 silent은 비슷한 숫자로 인코딩. \n",
        "\n",
        "NN의 입력으론 부적합.\n",
        "\n",
        "그래서 후술할 단어기반 tokenizer 이용.\n",
        "\n",
        "* 2. 단어 기반 인코딩\n",
        "\n",
        "I love my dog, I love my cat에서\n",
        "\n",
        "tokenizer 쓰면\n",
        "\n",
        "{\"I\":1, 'love':2, 'my' : 3 , 'dog' : 4, 'cat' : 5 }\n",
        "가 되는데 이런걸 이용.\n",
        "\n",
        "*num_words* = 몇개를 이용할 건지.\n",
        "\n",
        "*fit_on_text* = 문자 데이터 입력 -> 리스트\n",
        "\n",
        "*word_index* = tokenizer의 속성. {word : num} 형태의 dict반환\n",
        "\n",
        "---\n",
        "\n",
        "**texts_to_sequences** : 텍스트 안의 단어를 숫자 시퀀스로 변환.\n",
        "\n",
        "\n",
        "**<font color = \"red\">oov_otken</font> = \"OOV\"**\n",
        "- token화 안된것들을 \"<OOV>\"로 표시.\n",
        "\n",
        "*pad_sequence* : 다른 개수의 단어인 문장들을 같은 길이로!\n",
        "\n",
        "- padding: padding type지정.\n",
        "- maxlen : sequence의 최대 길이 제한.\n",
        "- truncating : 잘라낼 위치 지정.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKfH2fN4oCSZ"
      },
      "source": [
        "#Using LSTM\n",
        "\n",
        "token = text.Tokenizer(num_words = None)\n",
        "max_len = 70\n",
        "\n",
        "token.fit_on_texts(list(xtrain)+list(xvalid))\n",
        "xtrain_seq = token.texts_to_sequences(xtrain)\n",
        "xvalid_seq = token.texts_to_sequences(xvalid)\n",
        "\n",
        "#Zero pad the sequencees\n",
        "\n",
        "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen = max_len)\n",
        "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen = max_len)\n",
        "\n",
        "word_index = token.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfrqQcUzeIVV",
        "outputId": "712375c4-d9fa-4b72-8072-f5cd95ab9ccb"
      },
      "source": [
        "token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f7116447dd0>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vpYAUkAoCU_",
        "outputId": "d6c0142d-895b-4136-e826-f51b2ac661da"
      },
      "source": [
        "#Create an embedding mtx for words in the dataset\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
        "\n",
        "#embeddings_index는 glove vector 학습할 때 이미 가져왔었음!\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25943/25943 [00:00<00:00, 208090.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JypOl9E_kIT_"
      },
      "source": [
        "# Some findings\n",
        "\n",
        "- Embedding의 input_length에 맞게 Input 레이어를 설정해줘야함.\n",
        "\n",
        "- RNN model은 SpatialDroupout1D랑 recurrent_dropout이라는걸 쓰나봄.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgmcRxq0h4ms"
      },
      "source": [
        "from tensorflow.keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n61NGWUGoCXu",
        "outputId": "c34ad11a-1720-4ba2-f995-b7cf1bae28a6"
      },
      "source": [
        "#A simple LSTM with glove\n",
        "\n",
        "\n",
        "input_layer = Input(shape = (max_len,))\n",
        "embedding = Embedding(len(word_index)+1, 300, weights = [embedding_matrix], \n",
        "              input_length = max_len, trainable = False)(input_layer)\n",
        "x = SpatialDropout1D(0.3)(embedding)\n",
        "x = LSTM(100, dropout = 0.3, recurrent_dropout = 0.3)(x)\n",
        "\n",
        "x = Dense(1024, activation = \"relu\")(x)\n",
        "x = Dropout(0.8)(x)\n",
        "\n",
        "x = Dense(1024, activation = \"relu\")(x)\n",
        "x = Dropout(0.8)(x)\n",
        "\n",
        "x = Dense(3)(x)\n",
        "output = Activation(\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs = input_layer, outputs = output)\n",
        "\n",
        "#Fit the model with early stopping callback\n",
        "\n",
        "earlystop = EarlyStopping(monitor = \"val_loss\", min_delta = 0, patience = 3, verbose = 0, mode = \"auto\")\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 70)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 70, 300)           7783200   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_4 (Spatial (None, 70, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1024)              103424    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 3)                 3075      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 9,099,699\n",
            "Trainable params: 1,316,499\n",
            "Non-trainable params: 7,783,200\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfEYwk1fh_b1"
      },
      "source": [
        "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2zOxjKxoCZo",
        "outputId": "bf3e3c1c-c98e-45e5-ad25-9a20f3dad472"
      },
      "source": [
        "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(xvalid_pad, yvalid_enc),\n",
        "          \n",
        "          callbacks = [earlystop]\n",
        "          \n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "35/35 [==============================] - 56s 1s/step - loss: 1.0618 - val_loss: 0.9179\n",
            "Epoch 2/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.9016 - val_loss: 0.7536\n",
            "Epoch 3/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.8102 - val_loss: 0.7282\n",
            "Epoch 4/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.7834 - val_loss: 0.6963\n",
            "Epoch 5/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.7588 - val_loss: 0.6631\n",
            "Epoch 6/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.7420 - val_loss: 0.6498\n",
            "Epoch 7/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.7208 - val_loss: 0.6305\n",
            "Epoch 8/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.6963 - val_loss: 0.6297\n",
            "Epoch 9/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.6853 - val_loss: 0.6234\n",
            "Epoch 10/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.6641 - val_loss: 0.6085\n",
            "Epoch 11/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.6338 - val_loss: 0.5787\n",
            "Epoch 12/100\n",
            "35/35 [==============================] - 53s 2s/step - loss: 0.6273 - val_loss: 0.5642\n",
            "Epoch 13/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.6125 - val_loss: 0.5466\n",
            "Epoch 14/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.5875 - val_loss: 0.5480\n",
            "Epoch 15/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.5813 - val_loss: 0.5344\n",
            "Epoch 16/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.5545 - val_loss: 0.5224\n",
            "Epoch 17/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.5342 - val_loss: 0.5301\n",
            "Epoch 18/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.5271 - val_loss: 0.5217\n",
            "Epoch 19/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.5229 - val_loss: 0.5210\n",
            "Epoch 20/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.5075 - val_loss: 0.5122\n",
            "Epoch 21/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.4944 - val_loss: 0.5133\n",
            "Epoch 22/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.4837 - val_loss: 0.5035\n",
            "Epoch 23/100\n",
            "35/35 [==============================] - 51s 1s/step - loss: 0.4775 - val_loss: 0.5113\n",
            "Epoch 24/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.4565 - val_loss: 0.5184\n",
            "Epoch 25/100\n",
            "35/35 [==============================] - 52s 1s/step - loss: 0.4567 - val_loss: 0.5422\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f710f319950>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHKSFbyHQTum"
      },
      "source": [
        "**Bi-directional LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21UiE2lriFPW",
        "outputId": "6a5754c9-7083-449e-d2b9-c3c5350f1f80"
      },
      "source": [
        "input_layer = Input(shape = (max_len,))\n",
        "embedding = Embedding(  \n",
        "    len(word_index)+1, 300, weights = [embedding_matrix],\n",
        "    input_length = max_len,trainable = False\n",
        "    )(input_layer) \n",
        "x = SpatialDropout1D(0.3)(embedding)\n",
        "x = Bidirectional(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))(x)\n",
        "\n",
        "x = Dense(1024, activation = \"relu\")(x)\n",
        "x = Dropout(0.8)(x)\n",
        "\n",
        "x = Dense(1024, activation = \"relu\")(x)\n",
        "x = Dropout(0.8)(x)\n",
        "\n",
        "output = Dense(3, activation = \"softmax\")(x)\n",
        "\n",
        "model = Model(inputs = input_layer, outputs = output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 70)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_7 (Embedding)      (None, 70, 300)           7783200   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_6 (Spatial (None, 70, 300)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 600)               1442400   \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1024)              615424    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 3)                 3075      \n",
            "=================================================================\n",
            "Total params: 10,893,699\n",
            "Trainable params: 3,110,499\n",
            "Non-trainable params: 7,783,200\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUCozKZvo6XZ",
        "outputId": "917e044b-bc33-46da-d98d-14166821605a"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Fit the model with early stopping callback\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "35/35 [==============================] - 295s 8s/step - loss: 1.0345 - val_loss: 0.8732\n",
            "Epoch 2/100\n",
            "35/35 [==============================] - 286s 8s/step - loss: 0.8623 - val_loss: 0.7427\n",
            "Epoch 3/100\n",
            "35/35 [==============================] - 280s 8s/step - loss: 0.7990 - val_loss: 0.7040\n",
            "Epoch 4/100\n",
            "35/35 [==============================] - 284s 8s/step - loss: 0.7741 - val_loss: 0.6966\n",
            "Epoch 5/100\n",
            "35/35 [==============================] - 288s 8s/step - loss: 0.7564 - val_loss: 0.6581\n",
            "Epoch 6/100\n",
            "35/35 [==============================] - 278s 8s/step - loss: 0.7318 - val_loss: 0.6454\n",
            "Epoch 7/100\n",
            "35/35 [==============================] - 276s 8s/step - loss: 0.7098 - val_loss: 0.6278\n",
            "Epoch 8/100\n",
            "35/35 [==============================] - 279s 8s/step - loss: 0.6814 - val_loss: 0.6324\n",
            "Epoch 9/100\n",
            "35/35 [==============================] - 278s 8s/step - loss: 0.6547 - val_loss: 0.6083\n",
            "Epoch 10/100\n",
            "35/35 [==============================] - 286s 8s/step - loss: 0.6268 - val_loss: 0.5977\n",
            "Epoch 11/100\n",
            "35/35 [==============================] - 295s 8s/step - loss: 0.6129 - val_loss: 0.6050\n",
            "Epoch 12/100\n",
            "35/35 [==============================] - 281s 8s/step - loss: 0.5777 - val_loss: 0.5470\n",
            "Epoch 13/100\n",
            "23/35 [==================>...........] - ETA: 1:37 - loss: 0.5671"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p16JwtfEiFSR"
      },
      "source": [
        "# Ensemble\n",
        "\n",
        "Abhishek은 그가 직접 만든 pysembler를 이용하고 있다.\n",
        "logging 모듈을 공부해볼법한데, 일단은 패스!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrljxI-liFVG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a44a5bb3-ad97-4455-e445-2f0cf23eb437"
      },
      "source": [
        "#This is the main ensembling class.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "\n",
        "import pandas as pd\n",
        "import os \n",
        "import sys\n",
        "import logging\n",
        "\n",
        "\n",
        ")\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-892971e75580>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0OmBRcfiFX_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I--humZ1iFa9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_e3X96WiFeW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYVLBgyniFhW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXPLKTIIiFke"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCBMKWw0iFnN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiFyBbpGiFqR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZOm81DiFs9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW47IY5giFv1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf_MtdokiFyn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR0iSz_giF1g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU-8dnOkiF4A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWH6GEUiF6p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}